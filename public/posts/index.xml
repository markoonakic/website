<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xml:lang="en-us" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Marko Nakić</title>
        <link>https://markonakic.xyz</link>
        <description>Blog posts by Marko Nakić</description>
        <language>en-us</language>
        <lastBuildDate>Fri, 05 Dec 2025 15:28:51 &#43;0100</lastBuildDate>
        <pubDate>Fri, 05 Dec 2025 00:00:00 &#43;0000</pubDate>
        <ttl>60</ttl>
        <atom:link href="https://markonakic.xyz/posts/index.xml" rel="self" type="application/rss+xml"/>
        <generator>Hugo 0.152.2</generator>
        <managingEditor>marko@markonakic.xyz (Marko Nakić)</managingEditor>
        <webMaster>marko@markonakic.xyz (Marko Nakić)</webMaster>
        <item>
            <title>Homelab Remote Access Architecture</title>
            <link>https://markonakic.xyz/posts/homelab-remote-access-architecture/</link>
            <guid isPermaLink="true">https://markonakic.xyz/posts/homelab-remote-access-architecture/</guid>
            <pubDate>Fri, 05 Dec 2025 00:00:00 &#43;0000</pubDate>
            <author>marko@markonakic.xyz (Marko Nakić)</author><description>This post explains how I remotely access my Kubernetes homelab cluster through a secure architecture using Traefik, MetalLB, and cert-manager.&amp;#xA;Architecture Overview %%{init: { …</description><content:encoded><![CDATA[<p>This post explains how I remotely access my Kubernetes homelab cluster through a secure architecture using Traefik, MetalLB, and cert-manager.</p><h2 id=architecture-overview>Architecture Overview</h2><div class=mermaid>%%{init: {
'theme':'base',
'themeVariables': {
'primaryColor':'#3c3836',
'primaryTextColor':'#ebdbb2',
'primaryBorderColor':'#d65d0e',
'lineColor':'#fe8019',
'secondaryColor':'#504945',
'tertiaryColor':'#665c54',
'background':'#282828',
'mainBkg':'#282828',
'secondBkg':'#282828',
'textColor':'#ebdbb2',
'nodeBorder':'#d65d0e',
'clusterBkg':'#282828',
'clusterBorder':'#fe8019',
'edgeLabelBackground':'#282828',
'edgeLabelText':'#ebdbb2'
},
'flowchart': {
'nodeSpacing': 100,
'rankSpacing': 120,
'curve': 'basis'
}
}}%%
graph TB
Internet((Internet))
Internet -->|"HTTP/HTTPS (80/443)&lt;br/>DERP (3478)"| Router[Router&lt;br/>192.168.0.1]
Router -->|"Port Forward&lt;br/>All Traffic → 192.168.0.220"| MetalLB
MetalLB[MetalLB&lt;br/>Load Balancer&lt;br/>192.168.0.220]
subgraph K8sCluster["Kubernetes Cluster"]
direction TB
MetalLB -->|"Routes Traffic"| TraefikSvc
subgraph Ingress["Ingress Layer"]
direction TB
TraefikSvc[Traefik Service]
TraefikSvc --> Traefik[Traefik&lt;br/>Ingress Controller]
end
subgraph CertManagement["Certificate Management"]
direction TB
CertManager["cert-manager&lt;br/>(DNS-01)"]
Reflector["Reflector&lt;br/>(Sync Secrets)"]
CertManager -->|"Creates&lt;br/>*.sarma.love"| Reflector
end
Reflector -.->|"Syncs TLS"| IngressResources
subgraph IngressResources["Ingress Resources"]
direction TB
GrafanaIng["grafana.sarma.love&lt;br/>(Ingress)"]
HeadscaleIng["headscale.sarma.love&lt;br/>(Ingress)"]
GlanceIng["glance.sarma.love&lt;br/>(Ingress)"]
end
subgraph Applications["Application Services"]
direction TB
subgraph MonNS["monitoring namespace"]
Grafana[Grafana]
end
subgraph HeadNS["headscale namespace"]
Headscale[Headscale&lt;br/>Control Server]
Headplane[Headplane UI]
end
subgraph DashNS["dashboard namespace"]
Glance[Glance&lt;br/>Dashboard]
end
end
GrafanaIng --> Grafana
HeadscaleIng --> Headscale
HeadscaleIng --> Headplane
GlanceIng --> Glance
Traefik -->|"Routes by&lt;br/>Hostname"| IngressResources
subgraph Storage["Storage Layer"]
Longhorn[(Longhorn&lt;br/>Distributed Storage)]
end
Grafana -.->|"Persistent&lt;br/>Storage"| Longhorn
Headscale -.->|"Persistent&lt;br/>Storage"| Longhorn
Glance -.->|"Persistent&lt;br/>Storage"| Longhorn
end
classDef internetStyle fill:#076678,stroke:#458588,stroke-width:3px,color:#ebdbb2
classDef routerStyle fill:#af3a03,stroke:#d65d0e,stroke-width:3px,color:#ebdbb2
classDef lbStyle fill:#9d0006,stroke:#cc241d,stroke-width:3px,color:#ebdbb2
classDef traefikStyle fill:#8f3f71,stroke:#b16286,stroke-width:2px,color:#ebdbb2
classDef certStyle fill:#427b58,stroke:#689d6a,stroke-width:2px,color:#ebdbb2
classDef storageStyle fill:#b57614,stroke:#d79921,stroke-width:2px,color:#ebdbb2
classDef appStyle fill:#076678,stroke:#458588,stroke-width:2px,color:#ebdbb2
classDef ingressStyle fill:#79740e,stroke:#98971a,stroke-width:2px,color:#ebdbb2
class Internet internetStyle
class Router routerStyle
class MetalLB lbStyle
class Traefik,TraefikSvc traefikStyle
class CertManager,Reflector certStyle
class Longhorn storageStyle
class Grafana,Headscale,Headplane,Glance appStyle
class GrafanaIng,HeadscaleIng,GlanceIng ingressStyle</div><h2 id=traffic-flow>Traffic Flow</h2><p>All external traffic follows the same path through the ingress layer:</p><ol><li><p><strong>Internet → Router</strong>: All requests (HTTP/HTTPS on 80/443 and Headscale DERP on 3478) arrive at the home router</p></li><li><p><strong>Router → MetalLB</strong>: Port forwarding sends all traffic to the MetalLB virtual IP (192.168.0.220)</p></li><li><p><strong>MetalLB → Traefik</strong>: MetalLB load balancer distributes traffic to Traefik service across both cluster nodes</p></li><li><p><strong>Traefik → Ingress Resources</strong>: Traefik reads Kubernetes Ingress resources and routes based on hostname:</p><ul><li><code>grafana.sarma.love</code> → Grafana service in monitoring namespace</li><li><code>headscale.sarma.love</code> → Headscale/Headplane services in headscale namespace</li><li><code>glance.sarma.love</code> → Glance dashboard in dashboard namespace</li></ul></li></ol><h2 id=certificate-management>Certificate Management</h2><p>TLS certificates are automatically managed through a DNS-01 challenge workflow:</p><ol><li><p><strong>cert-manager</strong> obtains a wildcard certificate (<code>*.sarma.love</code>) from Let&rsquo;s Encrypt using DNS-01 validation</p></li><li><p><strong>Reflector</strong> automatically syncs the <code>sarma-wildcard-tls</code> secret to all namespaces that need it (monitoring, headscale, dashboard)</p></li><li><p><strong>Traefik</strong> terminates TLS connections using these synced certificates from each namespace&rsquo;s Ingress resource</p></li></ol><h2 id=headscale-integration>Headscale Integration</h2><p>Headscale runs inside the cluster as a self-hosted Tailscale control server. Unlike typical setups, it doesn&rsquo;t receive direct port forwarding—instead, all traffic (including DERP relay on port 3478) flows through the same Traefik ingress path, ensuring consistent routing and TLS termination.</p><h2 id=infrastructure>Infrastructure</h2><p>The cluster runs on two nodes:</p><ul><li><strong>Node 1</strong> (192.168.0.200): Control plane + worker node</li><li><strong>Node 2</strong> (192.168.0.201): Worker node</li></ul><p>Storage is provided by <strong>Longhorn</strong>, which provides distributed persistent volumes across both nodes for applications that require data persistence.</p>]]></content:encoded>
            <category>homelab</category>
            <category>kubernetes</category>
            <category>traefik</category>
            <category>architecture</category>
        </item>
        <item>
            <title>Remote Access Architecture for my Kubernetes Cluster</title>
            <link>https://markonakic.xyz/posts/remote-access/</link>
            <guid isPermaLink="true">https://markonakic.xyz/posts/remote-access/</guid>
            <pubDate>Fri, 05 Dec 2025 00:00:00 &#43;0000</pubDate>
            <author>marko@markonakic.xyz (Marko Nakić)</author><description>Why provide oneself remote homelab access? I think the concept of remotely accessing my homelab is really cool. Think about it - you can be anywhere in the world, and all you would need to connect to …</description><content:encoded><![CDATA[<h1 id=why-provide-oneself-remote-homelab-access>Why provide oneself remote homelab access?</h1><p>I think the concept of remotely accessing my homelab is really cool.
Think about it - you can be anywhere in the world, and all you would need to connect to your homelab is an internet connection.
So you take some machines, place them somewhere physicaly in the world, configure your choice of software on it, and then you can go anywhere knowing you&rsquo;re just a click away from accessing those same machines from a device of your choosing.</p><p>Not only is it cool, but it is incredibaly useful too.
You can access all the services you normally self-host at home from anywhere.
You can tinker with your server whenever you want.
You can do some remote debugging.</p><p>In essence, you can do whatever you want with you server or other devices in your network, from anywhere!</p><h1 id=my-experience-with-remotely-accessing-my-homelab>My experience with remotely accessing my homelab</h1><p>The first homelab I had was running Debian server and I ran services within Docker containers.</p><p>I gained remote access by running a Wireguard VPN service inside my homelab, which allowed me to securely connecttrough and encrypted tunnel.
This solution was simple and effective, without many layers to it.
I just created a docker-compose file, ran the container, forwarded the ports, and then it didn&rsquo;t work because i hadn&rsquo;t set up the networking properly :).
After learning a lot about networking and how everything works, this solution served me well for a while.</p><p>However, I realized the limitations this kind of setup brings.
There were no fancy UIs, all the connections were key pair based, so it was a bit of a hassle to manage.
I never really set up any kind of controller so all of my services were just exposed by IP and port to my whole local network, not quite the optimal setup, but I gained a lot of knowlegde and experience doing it.</p><p>Fast forward, the time had come for some renovation.
My Debian setup had served me well and it was time to move on to something more advanced.</p><p>In comes Talos linux, the distribution built to run Kubernetes.
With this transition many more layers were added, which greatly complicated the infrastructure setup.</p><p>I first needed to decide on what solution to use this time.
I had been reading a lot about the Tailscale protocol and i really liked the concept.
But with Tailscale i would have to go trough their servers in order to establish a connection.
This would violate the self-imposed constraints I had adheered by since first setting up my homelab.
I had strongly avoided routing my traffick trough any external services, the goal was to keep as much independence and privacy as possible.</p><p>But then i ran accross a project called Headscale.
An open source, self-hosted implementation of the Tailscale control server.
This would allow me to self-host my own server side for the implementation of the Tailscale protocol.
That&rsquo;s when i decided that was the solution i would go with.</p><h1 id=mappping-out-the-way-i-would-implement-headscale>Mappping out the way i would implement Headscale</h1><p>At first, I was oblivious to the amout of layers and complicatios the Talos + Kubernes setup would bring.</p><p>I knew i would need an ingress controller, for that I had already decided i would use Traefik.</p><p>Then I realized that i can&rsquo;t just forward traffic from my router to a single machines IP address, since now i had a multi node cluster, and i couldn&rsquo;t know which node Headscale or any other service would be running on.</p><p>There was the option of hard-coding services to run on specific nodes, but i didn&rsquo;t like that idea as i wanted a flexible and scalable setup.</p><p>That&rsquo;s when I found out i needed a load balancer implementation, so that i can assign real IP addesses from the main network to the services living in the cluster.</p><p>The idea then was to route the traffic from the router to Traefik using the MetalLB load balancer.</p><p>After reading some more documentation, I found out i needed a domain and a properly signed certificate in order for Headscale to work. That was fine, since i had already planned to have specfic subdomains for all my services, that were encrypted via https.</p><p>I figured I can obtain certificates via Let&rsquo;s encrypt, using an ACME challenge like HTTP‑01 or DNS‑01.</p><p>Then I realized that the manual managing of these certificates and secrets would be quite a hassle, and that&rsquo;s when I found out about the Kubernetes Reflector.</p><p>Reflector is a controller that can automatically replicate Kubernetes secrets and ConfigMaps across namespaces.
I decided to use it in order to sync the wildcard certificate secret with other namespaces, so that every ingress can reference the same cert by default.</p><p>There were also some atempts at setting up Pi-hole.
I tried setting up a local DNS server in order for all my services to be accessble only trough the Tailscale network, to avoid having to expose all me services externaly.</p><p>This idea was later scrapped when I found out Headscale already has a build in DNS system called MagicDNS. This is then what I used instead of Pi-hole.</p><p>Of course, all of these decisions and conclusions did not come out of thin air, but were the result of continuous trial, error, and research.
In the end I wound up with a setup I wouldn&rsquo;t be able to wrap me head around when I started.</p><h1 id=the-architecture-of-my-current-setup>The architecture of my current setup</h1>]]></content:encoded>
            <category>Remote Access</category>
            <category>Kubernetes</category>
            <category>Homelab</category>
        </item>
        <item>
            <title>Test: Mermaid &#43; Code Highlighting</title>
            <link>https://markonakic.xyz/posts/test-mermaid-and-code/</link>
            <guid isPermaLink="true">https://markonakic.xyz/posts/test-mermaid-and-code/</guid>
            <pubDate>Fri, 05 Dec 2025 00:00:00 &#43;0000</pubDate>
            <author>marko@markonakic.xyz (Marko Nakić)</author><description>Testing Both Features 1. Code Highlighting (should work normally) Here’s some Python code:&amp;#xA;def fibonacci(n): if n &amp;lt;= 1: return n return fibonacci(n-1) &#43; fibonacci(n-2) # Test …</description><content:encoded><![CDATA[<h2 id=testing-both-features>Testing Both Features</h2><h3 id=1-code-highlighting-should-work-normally>1. Code Highlighting (should work normally)</h3><p>Here&rsquo;s some Python code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>fibonacci</span><span class=p>(</span><span class=n>n</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>n</span> <span class=o>&lt;=</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>n</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>fibonacci</span><span class=p>(</span><span class=n>n</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=o>+</span> <span class=n>fibonacci</span><span class=p>(</span><span class=n>n</span><span class=o>-</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Test</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>fibonacci</span><span class=p>(</span><span class=mi>10</span><span class=p>))</span>
</span></span></code></pre></div><p>Here&rsquo;s some bash:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=cp>#!/bin/bash
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=nb>echo</span> <span class=s2>&#34;Hello, world!&#34;</span>
</span></span><span class=line><span class=cl><span class=k>for</span> i in <span class=o>{</span>1..5<span class=o>}</span><span class=p>;</span> <span class=k>do</span>
</span></span><span class=line><span class=cl>    <span class=nb>echo</span> <span class=s2>&#34;Number: </span><span class=nv>$i</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=k>done</span>
</span></span></code></pre></div><h3 id=2-mermaid-diagram-should-render-as-diagram>2. Mermaid Diagram (should render as diagram)</h3><div class=mermaid>%%{init: {
'theme':'base',
'themeVariables': {
'primaryColor':'#3c3836',
'primaryTextColor':'#ebdbb2',
'primaryBorderColor':'#d65d0e',
'lineColor':'#fe8019',
'background':'#282828',
'mainBkg':'#282828',
'secondBkg':'#282828',
'textColor':'#ebdbb2',
'clusterBkg':'#282828'
}
}}%%
graph LR
A[Code Blocks] --> B{Still Work?}
B -->|Yes| C[Syntax Highlighting]
B -->|Yes| D[Mermaid Diagrams]
C --> E[Both Work Together!]
D --> E
classDef successStyle fill:#689d6a,stroke:#8ec07c,color:#282828
class E successStyle</div><h3 id=3-more-code-to-verify-it-continues-working>3. More Code (to verify it continues working)</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=c1>// JavaScript example
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kr>const</span> <span class=nx>greet</span> <span class=o>=</span> <span class=p>(</span><span class=nx>name</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nx>console</span><span class=p>.</span><span class=nx>log</span><span class=p>(</span><span class=sb>`Hello, </span><span class=si>${</span><span class=nx>name</span><span class=si>}</span><span class=sb>!`</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nx>greet</span><span class=p>(</span><span class=s2>&#34;Mermaid User&#34;</span><span class=p>);</span>
</span></span></code></pre></div><p>If you see:</p><ul><li>✅ <strong>Syntax highlighted code</strong> (with colors)</li><li>✅ <strong>Rendered Mermaid diagram</strong> (not code text)</li></ul><p>Then everything works correctly!</p>]]></content:encoded>
            <category>test</category>
        </item>
        <item>
            <title>Installing Talos Linux</title>
            <link>https://markonakic.xyz/posts/talos-install/</link>
            <guid isPermaLink="true">https://markonakic.xyz/posts/talos-install/</guid>
            <pubDate>Mon, 24 Nov 2025 00:00:00 &#43;0000</pubDate>
            <author>marko@markonakic.xyz (Marko Nakić)</author><description>What is Talos? Talos Linux is an open‑source Linux distribution built specifically to run Kubernetes. The OS is immutable, and is managed via a declarative API which is protected with mutual TLS and …</description><content:encoded><![CDATA[<h1 id=what-is-talos>What is Talos?</h1><p>Talos Linux is an open‑source Linux distribution built specifically to run Kubernetes.
The OS is immutable, and is managed via a declarative API which is protected with mutual TLS and role‑based access control.
It does not have an interactive shell nor does it support SSH, which greatly reduces the surface area for attacks.
The whole OS is defined as infrastructure-as-code, which makes it easily reproducible and minimizes configuration drift.</p><h1 id=installation>Installation</h1><h2 id=flashing-the-iso>Flashing the ISO</h2><p>The first step is grabbing the right ISO for your machine from <a href=https://github.com/siderolabs/talos/releases>here</a>. Then flash the ISO and boot into the installation medium.</p><p>At this point after the OS has booted Talos will be in <strong>maintenance mode</strong>.</p><p>While Talos is in maintenance mode it is important to run all <code>talosctl</code> commands with the <code>--insecure</code> flag.
Once we apply the configuration and the OS is installed properly Talos will stop accepting the <code>--insecure</code> flag and we will authenticate using the <code>talosconfig</code> file.</p><p>Now write down or remember the IP address of the Talos machine, which is displayed in the dashboard.</p><h2 id=inspecting-the-disks>Inspecting the disks</h2><p>Ensure <code>talosctl</code> is installed on your laptop (and ideally matches the ISO version).</p><p>In a terminal on your laptop (or any other computer on the same network as the Talos machine) we need to run a command that will show us the drives available to the Talos machine.</p><p>We can do this using a helper variable for convenience:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>NODE_IP=192.168.221.143   # Your talos machine IP here
</span></span></code></pre></div><p>List disks:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>talosctl get disks -n &#34;$NODE_IP&#34; --insecure
</span></span></code></pre></div><h2 id=generate-config-with-the-right-install-disk>Generate config with the right install disk</h2><p>Now once you have listed your disks you will need to select the one you want Talos to be installed to.</p><p>Helper variable for convenience:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>INSTALL_DISK=/dev/sda   # Replace with disk you decided on
</span></span></code></pre></div><p>Generate config:</p><p>Keep in mind this command will generate <code>controlplane.yaml</code>, <code>worker.yaml</code> and <code>talosconfig</code> in your current working directory.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>talosctl gen config CLUSTER_NAME https://$NODE_IP:6443 \
</span></span><span class=line><span class=cl>  --install-disk &#34;$INSTALL_DISK&#34;
</span></span></code></pre></div><p>The <code>--install-disk</code> flag baked the disk you selected into the generated <code>controlplane.yaml</code> and <code>worker.yaml</code>.</p><p>You can replace <code>CLUSTER_NAME</code> with any cluster name you like.</p><h2 id=enable-workloads-on-the-control-plane>Enable workloads on the control plane</h2><p><strong>NOTE</strong> - This step is optional, intended for clusters with a low number of nodes.</p><p>Open <code>controlplane.yaml</code> and in the <code>cluster:</code> section add:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>cluster:
</span></span><span class=line><span class=cl>  allowSchedulingOnControlPlanes: true
</span></span></code></pre></div><p>This will allow pods to run on the control-plane node.</p><h2 id=apply-the-config>Apply the config</h2><p>From the directory where <code>controlplane.yaml</code> and <code>talosconfig</code> were generated, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>talosctl apply-config --insecure \
</span></span><span class=line><span class=cl>  --nodes &#34;$NODE_IP&#34; \
</span></span><span class=line><span class=cl>  --file controlplane.yaml
</span></span></code></pre></div><p>The <code>apply-config</code> command sends your config to the ISO-booted node and instructs it to install Talos to the configured disk.</p><p><strong>What you should see on the node</strong>:</p><p>Logs of the image being written to the disk you defined in <code>controlplane.yaml</code>.</p><p>At some point the screen will turn black, and the system will reboot.
Wait until Talos finishes booting and displays the dashboard again.</p><p>Note down the IP to be used as <code>NODE_IP</code> (it may be the same as before, but confirm to be safe).</p><h2 id=point-talosctl-at-this-node-using-talosconfig>Point <code>talosctl</code> at this node using <code>talosconfig</code></h2><p>Now that the node is running from the disk with the keys we generated, stop using <code>--insecure</code> and instead use the <code>talosconfig</code> file that was created using <code>gen config</code>.</p><p>From the same directory, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>talosctl --talosconfig=./talosconfig config endpoint &#34;$NODE_IP&#34;
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>talosctl --talosconfig=./talosconfig config node &#34;$NODE_IP&#34;
</span></span></code></pre></div><p>This writes our nodes IP to the <code>endpoint</code> and <code>node</code> values, so that future <code>talosctl</code> calls know where to connect.</p><p>You can check the connectivity by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>talosctl --talosconfig=./talosconfig version
</span></span></code></pre></div><p>and</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>talosctl --talosconfig=./talosconfig service kubelet
</span></span></code></pre></div><h2 id=bootstrap-kubernetes>Bootstrap Kubernetes</h2><p>Now we need to bootstrap the Kubernetes control plane (etcd, API server, etcd membership).</p><p>To bootstrap, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>talosctl bootstrap \
</span></span><span class=line><span class=cl>  --nodes &#34;$NODE_IP&#34; \
</span></span><span class=line><span class=cl>  --endpoints &#34;$NODE_IP&#34; \
</span></span><span class=line><span class=cl>  --talosconfig=./talosconfig
</span></span></code></pre></div><p>This command is run <strong>once for the whole cluster</strong>, against <strong>one control-plane node</strong>.
Running it multiple times can break etcd.</p><p>It may take a while. If you get some transient TLS or connection error, just wait 30s and retry as the API finishes starting.</p><h2 id=fetch-kubeconfig-and-verify-the-cluster>Fetch <code>kubeconfig</code> and verify the cluster</h2><p>Once the bootstrap succeeds, grab <code>kubeconfig</code> so you can use <code>kubectl</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>talosctl kubeconfig \
</span></span><span class=line><span class=cl>  --nodes &#34;$NODE_IP&#34; \
</span></span><span class=line><span class=cl>  --endpoints &#34;$NODE_IP&#34; \
</span></span><span class=line><span class=cl>  --talosconfig=./talosconfig \
</span></span><span class=line><span class=cl>  kubeconfig
</span></span></code></pre></div><p>This will write a file called <code>kubeconfig</code> in the current working directory.</p><p>After that:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=k>export</span> <span class=n>KUBECONFIG</span><span class=o>=$</span><span class=n>PWD</span><span class=o>/</span><span class=n>kubeconfig</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>kubectl get nodes
</span></span></code></pre></div><p>After running this you should see your single node in the <code>Ready</code> state, typically with a name derived from your config.</p><p>If you set <code>allowSchedulingOnControlPlanes:true</code>, it will be scheduled for workloads.</p><p><strong>Congratulations!!!</strong></p><p>At this point you should have your Talos cluster up and running!</p><p>If you are currently setting up your homelab, i now recommend you going with a <strong>GitOps</strong> deployment strategy.</p><p>The two main tools for this are <strong>Flux CD</strong> and <strong>Argo CD</strong>.</p><p>For a simple enough homelab I recommend going with <strong>Flux CD</strong>.
It is lightweight and similar to vanilla Kubernetes.</p>]]></content:encoded>
            <category>Talos</category>
            <category>Kubernetes</category>
            <category>Homelab</category>
        </item>
        <item>
            <title>Remote decryption using Dropbear SSH</title>
            <link>https://markonakic.xyz/posts/dropbear-shh/</link>
            <guid isPermaLink="true">https://markonakic.xyz/posts/dropbear-shh/</guid>
            <pubDate>Mon, 10 Nov 2025 00:00:00 &#43;0000</pubDate>
            <author>marko@markonakic.xyz (Marko Nakić)</author><description>My first encounter with encryption The first time i had encountered encryption was when i was installing Debian on a machine that was meant to be my first homelab. In the disk partitiong segment of …</description><content:encoded><![CDATA[<h2 id=my-first-encounter-with-encryption>My first encounter with encryption</h2><p>The first time i had encountered encryption was when i was installing Debian on a machine that was meant to be my first homelab. In the disk partitiong segment of the installation process I was presented with the option to use LUKS encryption on my drives. Since i had not yet dabbled with the idea, but found it cool, i decided to go with it and encrypt my drives.</p><p>Since i don&rsquo;t keep the homelab machine at my own place, I had to setup remote access to it right away. And in the process i realized - i cannot remotely decrypt my drives if the machine ever powers off. After that i scoured the web for solutions to my problem and in the end i landed on Dropbear SSH.</p><h2 id=what-is-dropbear-shh-and-how-does-it-apply-to-this-use-case>What is Dropbear SHH and how does it apply to this use case</h2><p>Dropbear SSH is a lightweigh SSH server and client that is primarily used on embedded systems with low memory and processor resources.</p><p>Most systems have some sort of pre-boot environment. This userspace is loaded into the RAM so that the kernel can load drivers and logic that&rsquo;s needed to mount the real root filesystem. The Linux kernel is shipped with the initramfs filesystem (or equivalent) by default.</p><p>Dropbear SHH can be leveraged to gain access to this pre-boot filesystem before the drives are mounted. This allows us to remotely decrypt the drives of our machine.</p><h2 id=implementation>Implementation</h2><p><strong>NOTE</strong> - This implementation walkthrough will be for Debian and Debian based systems, but the same principles still apply to others.</p><h3 id=update-and-upgrade-your-machine>Update and upgrade your machine</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>sudo apt update
</span></span><span class=line><span class=cl>sudo apt upgrade
</span></span></code></pre></div><h3 id=install-dropbear>Install Dropbear</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>sudo apt install dropbear-initramfs
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl> sudo -i
</span></span><span class=line><span class=cl> cd /etc/dropbear/initramfs/
</span></span></code></pre></div><p>Because these files affect the boot image, they live under /etc. Using <code>sudo -i</code> we can get a clean root environment that behaves as if we physically logged in as root.</p><h3 id=configure-dropbearconf>Configure dropbear.conf</h3><p>Options:</p><ul><li><code>-I</code> : Disconnect the session if no traffic is transmitted or received in x seconds</li></ul><p>Auto-disconnecting inactive sessions reduces exposure in the early boot process.</p><ul><li><p><code>-j</code>: Disable ssh local port forwarding.</p></li><li><p><code>-k</code> : Disable remote port forwarding.</p></li></ul><p>Disabling port forwarding minimizes attack surface.</p><ul><li><code>-p</code> : Dropbear listen on this specified TCP port.</li></ul><p>I always use a non default port to avoid generic scaning noise.</p><ul><li><code>-s</code> : Disable password logins.</li></ul><p>Always disable password logins and use key pairs for authentication.</p><p>Example:</p><pre><code>DROPBEAR_OPTIONS=&quot;-I 239 -j -k -p 5768 -s&quot;
</code></pre><h3 id=set-early-boot-networking>Set early-boot networking</h3><p>Early boot networking needs to be set in order for Dropbear to accept SSH before userspace networking is up. This is done by injecting a static IPv4 configuration in the initramfs config file => /etc/initramfs-tools/initramfs.conf.</p><p>Format:</p><pre><code>IP=SERVER_IP::ROUTER_IP:NETMASK:SERVER_HOSTNAME
</code></pre><p>Example:</p><pre><code>IP=192.168.1.36::192.168.1.1:255.255.255.0:node2
</code></pre><h3 id=update-the-initramfs>Update the initramfs</h3><pre><code>sudo update-initramfs -u -v
</code></pre><p>This rebuilds the initramfs with the changes for the current kernel. This command must be run after every change to the initramfs config so that it takes affect at boot.</p><h3 id=create-the-keys>Create the keys</h3><h4 id=generate-a-new-client-key>Generate a new client key</h4><pre><code>ssh-keygen -t rsa -f ~/.ssh/dropbear
</code></pre><p>The <code>-t</code> flag specifies what type of key to generate, while the <code>-f</code> flag specifies the file name and path.</p><h4 id=copy-the-key-to-the-server>Copy the key to the server:</h4><pre><code>scp ~/.ssh/dropbear.pub marko@192.168.0.200:~/dropbear.pub
</code></pre><p>Using <code>scp</code> we can copy the public key over to the server.</p><h4 id=add-the-key-to-the-initramfs-authorized-keys>Add the key to the initramfs authorized keys:</h4><pre><code>cat /home/marko/dropbear.pub &gt;&gt; /etc/dropbear/initramfs/authorized_keys
</code></pre><p>The key needs to be present in the initramfs so the authentication can succeed during the pre-boot process.</p><h4 id=stop-being-root>Stop being root</h4><pre><code>exit
</code></pre><h4 id=update-initramfs-again>Update initramfs again</h4><pre><code>sudo update-initramfs -u
</code></pre><h3 id=making-an-alias-optional>Making an alias (optional)</h3><h4 id=edit-the-bashrc-on-the-client>Edit the bashrc on the client</h4><pre><code>vim ~/.bashrc
</code></pre><p>I suggest making an alias for ease of use.</p><h4 id=add-the-alias>Add the alias</h4><p>Format:</p><pre><code>alias &lt;Aliasname&gt;=&quot;ssh -i ~/.ssh/dropbear -p &lt;port&gt; -o 'HostKeyAlgorithms ssh-rsa' root@&lt;SERVER_IP&gt; 'echo -n &lt;DRIVE_ENCRYPTION_PASSWORD&gt; | cryptroot-unlock'&quot;
</code></pre><ul><li><code>-i</code> flag selects the private key we created</li><li><code>-p</code> flag specifies the dropbear port</li><li><code>-o</code> flag forces the use of the RSA algorithm with witch we crated the key pair</li><li><code>root@</code> ensures we connect as a root user to the server</li><li>The <code>echo</code> command is for ease of use and is optional, if not included you will have to provide the password manually to unlock</li></ul><p>Exemple:</p><pre><code>alias unlock=&quot;ssh -i ~/.ssh/dropbear -p 5768 -o 'HostKeyAlgorithms ssh-rsa' root@192.168.0.200 'echo -n test | cryptroot-unlock'&quot;
</code></pre><h4 id=source-the-bashrc>Source the bashrc:</h4><pre><code>source .bashrc
</code></pre><p>This reloads the shell configuration so the alias is available immediately.</p><h3 id=reboot>Reboot:</h3><pre><code>sudo reboot now
</code></pre><p>Time to reboot the machine and test it out!</p><h2 id=try-to-unlock-the-server-with-your-alias>Try to unlock the server with your alias:</h2><pre><code>unlock
</code></pre><h2 id=manual-unlock-with-no-alias>Manual unlock with no alias:</h2><pre><code>ssh -i ~/.ssh/dropbear -p &lt;port&gt; -o &quot;HostKeyAlgorithms ssh-rsa&quot; root@&lt;SERVER_IP&gt;
cryptroot-unlock
</code></pre>]]></content:encoded>
            <category>Linux</category>
            <category>Encryption</category>
            <category>Remote Access</category>
        </item>
    </channel>
</rss>
